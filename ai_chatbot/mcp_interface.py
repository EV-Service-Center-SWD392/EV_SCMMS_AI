"""
MCP Interface for EV SCMMS AI Chatbot
Handles Gemini function calling for conversational AI interactions.
"""
import os
import json
import asyncio
import subprocess
from datetime import datetime
from typing import Dict, List, Any, Optional
import google.generativeai as genai
from dotenv import load_dotenv

# Load environment from shared config
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), '..', 'shared', 'config.env'))

# Configure Gemini
genai.configure(api_key=os.getenv('GEMINI_API_KEY'))

class ConversationManager:
    """Manages conversation history and context."""
    
    def __init__(self):
        self.conversations: Dict[str, List[Dict]] = {}
    
    def add_message(self, conversation_id: str, role: str, content: str, function_calls: List = None):
        """Add a message to conversation history."""
        if conversation_id not in self.conversations:
            self.conversations[conversation_id] = []
        
        message = {
            "timestamp": datetime.now().isoformat(),
            "role": role,
            "content": content,
            "function_calls": function_calls or []
        }
        
        self.conversations[conversation_id].append(message)
    
    def get_conversation(self, conversation_id: str) -> List[Dict]:
        """Get conversation history."""
        return self.conversations.get(conversation_id, [])
    
    def get_context_summary(self, conversation_id: str) -> str:
        """Get minimal context to reduce tokens."""
        history = self.get_conversation(conversation_id)
        if not history or len(history) == 0:
            return ""
        
        # Only get last user message for minimal context
        last_msg = history[-1]
        if last_msg["role"] == "user":
            return f"Last: {last_msg['content'][:50]}"
        
        return ""

class GeminiMCPChatbot:
    """Gemini-powered chatbot with MCP function calling for EV SCMMS."""
    
    def __init__(self):
        # Function declarations for MCP tools
        self.function_declarations = [
            genai.protos.FunctionDeclaration(
                name="get_spare_parts",
                description=(
                    "T√¨m ki·∫øm ph·ª• t√πng xe ƒëi·ªán theo t√™n, nh√† s·∫£n xu·∫•t, ho·∫∑c lo·∫°i ph·ª• t√πng. "
                    "T·ª± ƒë·ªông t√¨m ki·∫øm ƒëa tr∆∞·ªùng v·ªõi ƒë·ªô ch√≠nh x√°c cao. "
                    "K·∫øt qu·∫£ bao g·ªìm: t√™n, lo·∫°i ph·ª• t√πng, gi√° hi·ªán t·∫°i, s·ªë l∆∞·ª£ng t·ªìn kho, v√† c√°c th√¥ng tin li√™n quan kh√°c. "
                    "S·ª≠ d·ª•ng h√†m n√†y khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ th√¥ng tin, t√¨nh tr·∫°ng, ho·∫∑c gi√° c·ªßa ph·ª• t√πng."
                ),
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "part_name": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="T·ª´ kh√≥a t√¨m ki·∫øm (t√™n, nh√† s·∫£n xu·∫•t, ho·∫∑c lo·∫°i ph·ª• t√πng)"
                        )
                    }
                )
            ),
            genai.protos.FunctionDeclaration(
                name="get_inventory",
                description=(
                    "Xem t√¨nh tr·∫°ng t·ªìn kho hi·ªán t·∫°i c·ªßa c√°c trung t√¢m b·∫£o d∆∞·ª°ng. "
                    "D√πng khi ng∆∞·ªùi d√πng mu·ªën bi·∫øt s·ªë l∆∞·ª£ng ph·ª• t√πng c√≥ s·∫µn."
                ),
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "center_id": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="ID trung t√¢m c·ª• th·ªÉ (t√πy ch·ªçn)"
                        )
                    }
                )
            ),
            genai.protos.FunctionDeclaration(
                name="get_usage_history",
                description=(
                    "L·∫•y l·ªãch s·ª≠ s·ª≠ d·ª•ng ph·ª• t√πng ƒë·ªÉ ph√¢n t√≠ch xu h∆∞·ªõng v√† d·ª± b√°o nhu c·∫ßu. "
                    "C√≥ th·ªÉ l·ªçc theo t√™n ph·ª• t√πng, ID ph·ª• t√πng v√† trung t√¢m, trong kho·∫£ng 1-24 th√°ng."
                ),
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "months": genai.protos.Schema(
                            type=genai.protos.Type.INTEGER,
                            description="S·ªë th√°ng l·ªãch s·ª≠ c·∫ßn xem (1-24)"
                        ),
                        "part_name": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="T√™n ph·ª• t√πng ƒë·ªÉ t√¨m ki·∫øm (t√πy ch·ªçn)"
                        ),
                        "spare_part_id": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="ID ph·ª• t√πng c·ª• th·ªÉ (t√πy ch·ªçn)"
                        ),
                        "center_id": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="ID trung t√¢m c·ª• th·ªÉ (t√πy ch·ªçn)"
                        )
                    }
                )
            ),
            genai.protos.FunctionDeclaration(
                name="forecast_demand",
                description=(
                    "D·ª± b√°o nhu c·∫ßu ph·ª• t√πng trong t∆∞∆°ng lai (1-12 th√°ng) b·∫±ng AI. "
                    "Lu√¥n g·ªçi function n√†y khi ng∆∞·ªùi d√πng y√™u c·∫ßu d·ª± b√°o. "
                    "N·∫øu kh√¥ng c√≥ t√™n ph·ª• t√πng th√¨ d·ª± b√°o t·ªïng th·ªÉ. N·∫øu c√≥ t√™n th√¨ d·ª± b√°o ri√™ng."
                ),
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "months": genai.protos.Schema(
                            type=genai.protos.Type.INTEGER,
                            description="S·ªë th√°ng c·∫ßn d·ª± b√°o (1-12)"
                        ),
                        "part_name": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="T√™n ph·ª• t√πng ƒë·ªÉ d·ª± b√°o (t√πy ch·ªçn)"
                        ),
                        "spare_part_id": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="ID ph·ª• t√πng c·ª• th·ªÉ (t√πy ch·ªçn)"
                        ),
                        "center_id": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="ID trung t√¢m c·ª• th·ªÉ (t√πy ch·ªçn)"
                        )
                    }
                )
            ),
            genai.protos.FunctionDeclaration(
                name="create_sparepart",
                description=(
                    "H·ªó tr·ª£ t·∫°o ph·ª• t√πng m·ªõi. Ph√¢n t√≠ch th√¥ng tin ng∆∞·ªùi d√πng cung c·∫•p v√† tr·∫£ v·ªÅ "
                    "danh s√°ch c√°c tr∆∞·ªùng c·∫ßn thi·∫øt ƒë·ªÉ t·∫°o ph·ª• t√πng. AI kh√¥ng t·ª± ƒë·ªông t·∫°o v√†o database."
                ),
                parameters=genai.protos.Schema(
                    type=genai.protos.Type.OBJECT,
                    properties={
                        "name": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="T√™n ph·ª• t√πng (t√πy ch·ªçn)"
                        ),
                        "unitPrice": genai.protos.Schema(
                            type=genai.protos.Type.NUMBER,
                            description="Gi√° ƒë∆°n v·ªã (t√πy ch·ªçn)"
                        ),
                        "manufacturer": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="Nh√† s·∫£n xu·∫•t (t√πy ch·ªçn)"
                        ),
                        "typeName": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="Lo·∫°i ph·ª• t√πng (t√πy ch·ªçn)"
                        ),
                        "vehicleModelId": genai.protos.Schema(
                            type=genai.protos.Type.INTEGER,
                            description="ID m·∫´u xe (t√πy ch·ªçn)"
                        ),
                        "centerName": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="T√™n trung t√¢m (t√πy ch·ªçn)"
                        ),
                        "description": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="M√¥ t·∫£ ph·ª• t√πng (t√πy ch·ªçn)"
                        ),
                        "partNumber": genai.protos.Schema(
                            type=genai.protos.Type.STRING,
                            description="M√£ ph·ª• t√πng (t√πy ch·ªçn)"
                        )
                    }
                )
            )
        ]
        
        # Gemini model with function calling
        self.model = genai.GenerativeModel(
            model_name="gemini-2.5-flash",
            tools=[genai.protos.Tool(function_declarations=self.function_declarations)],
            tool_config=genai.protos.ToolConfig(
                function_calling_config=genai.protos.FunctionCallingConfig(
                    mode=genai.protos.FunctionCallingConfig.Mode.AUTO
                )
            ),
            system_instruction="AI tr·ª£ l√Ω ph·ª• t√πng xe ƒëi·ªán EV Service Center. QUAN TR·ªåNG: Khi ng∆∞·ªùi d√πng n√≥i 'd·ª± b√°o' th√¨ LU√îN g·ªçi forecast_demand. N·∫øu c√≥ t√™n ph·ª• t√πng th√¨ truy·ªÅn part_name, n·∫øu kh√¥ng th√¨ ƒë·ªÉ tr·ªëng. Kh√¥ng h·ªèi l·∫°i ng∆∞·ªùi d√πng. C√°c function kh√°c: ph·ª• t√πng‚Üíget_spare_parts, t·ªìn kho‚Üíget_inventory, l·ªãch s·ª≠‚Üíget_usage_history.",
            generation_config={
                "temperature": 0.3,
                "top_p": 0.8,
                "max_output_tokens": 512  # Reduce output tokens
            }
        )
        
        # Fallback model without function calling
        self.fallback_model = genai.GenerativeModel(
            model_name="gemini-2.5-flash",
            system_instruction="AI tr·ª£ l√Ω th√¢n thi·ªán cho h·ªá th·ªëng qu·∫£n l√Ω ph·ª• t√πng xe ƒëi·ªán EV Service Center. Tr·∫£ l·ªùi c√°c c√¢u h·ªèi chung v·ªÅ xe ƒëi·ªán, b·∫£o d∆∞·ª°ng, v√† h·ªó tr·ª£ kh√°ch h√†ng.",
            generation_config={
                "temperature": 0.7,
                "top_p": 0.9,
                "max_output_tokens": 512
            }
        )
        
        # Conversation manager
        self.conversation_manager = ConversationManager()
        
        # MCP server process
        self.mcp_process = None
    
    def start_mcp_server(self):
        """Start the shared MCP server."""
        if self.mcp_process is None:
            print("üöÄ Starting shared MCP server...")
            shared_dir = os.path.join(os.path.dirname(__file__), '..', 'shared')
            self.mcp_process = subprocess.Popen(
                ["python", "true_mcp_server.py"],
                cwd=shared_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            import time
            time.sleep(2)
            print("‚úÖ Shared MCP server started")
    
    def stop_mcp_server(self):
        """Stop the MCP server."""
        if self.mcp_process:
            self.mcp_process.terminate()
            self.mcp_process.wait()
            self.mcp_process = None
            print("üîí MCP server stopped")
    
    async def call_mcp_function(self, function_name: str, arguments: dict):
        """Call MCP server function."""
        # Import shared database functions
        import sys
        sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))
        from db_connection import fetch
        
        if function_name == "get_spare_parts":
            part_name = arguments.get("part_name")
            
            # Filter out None values and "None" strings
            if part_name in [None, "None", ""]:
                part_name = None
            
            sql = """
            SELECT s.sparepartid, s.name, s.unitprice, s.manufacture, s.status,
                   i.centerid, i.quantity, i.minimumstocklevel, 
                   t.name as type_name, v.name as vehicle_model_name
            FROM sparepart_tuht s
            LEFT JOIN inventory_tuht i ON s.inventoryid = i.inventoryid
            LEFT JOIN spareparttype_tuht t ON s.typeid = t.typeid
            LEFT JOIN vehiclemodel v ON s.vehiclemodelid = v.modelid
            WHERE s.isactive = true
            """
            params = []
            
            if part_name:
                # Multi-field search: name, manufacture, type
                sql += " AND (s.name ILIKE %s OR s.manufacture ILIKE %s OR t.name ILIKE %s)"
                search_term = f"%{part_name}%"
                params.extend([search_term, search_term, search_term])
                sql += " ORDER BY CASE WHEN s.name ILIKE %s THEN 1 WHEN s.manufacture ILIKE %s THEN 2 ELSE 3 END, s.name"
                params.extend([search_term, search_term])
            else:
                sql += " ORDER BY s.name"
            
            sql += " LIMIT 20"
            rows = await fetch(sql, *params) if params else await fetch(sql)
            # Return comprehensive data from real Supabase database
            result_data = []
            for row in rows[:15]:  # Show up to 15 items
                result_data.append({
                    "id": row.get("sparepartid", ""),
                    "name": row.get("name", ""),
                    "price": row.get("unitprice", 0),
                    "manufacture": row.get("manufacture", ""),
                    "qty": row.get("quantity", 0),
                    "min_stock": row.get("minimumstocklevel", 0),
                    "center_id": row.get("centerid", ""),
                    "status": row.get("status", ""),
                    "type": row.get("type_name", ""),
                    "vehicle_model": row.get("vehicle_model_name", "")
                })
            return {"data": result_data, "count": len(result_data), "source": "supabase_real_data"}
        
        elif function_name == "get_inventory":
            center_id = arguments.get("center_id")
            
            # Filter out None values
            if center_id in [None, "None", ""]:
                center_id = None
            
            sql = """
            SELECT i.inventoryid, i.centerid, i.quantity, i.minimumstocklevel, i.status,
                   s.sparepartid, s.name as spare_part_name, s.unitprice, s.manufacture
            FROM inventory_tuht i
            LEFT JOIN sparepart_tuht s ON i.inventoryid = s.inventoryid
            WHERE i.isactive = true
            """
            params = []
            
            if center_id:
                sql += " AND i.centerid = %s"
                params.append(center_id)
            
            sql += " LIMIT 10"
            rows = await fetch(sql, *params) if params else await fetch(sql)
            # Real inventory data from Supabase
            inventory_data = []
            for row in rows[:8]:
                inventory_data.append({
                    "inventory_id": row.get("inventoryid", ""),
                    "center_id": row.get("centerid", ""),
                    "spare_part_id": row.get("sparepartid", ""),
                    "part_name": row.get("spare_part_name", ""),
                    "quantity": row.get("quantity", 0),
                    "min_stock": row.get("minimumstocklevel", 0),
                    "price": row.get("unitprice", 0),
                    "manufacture": row.get("manufacture", ""),
                    "status": row.get("status", "")
                })
            return {"data": inventory_data, "count": len(inventory_data), "source": "supabase_real_data"}
        
        elif function_name == "get_usage_history":
            months = arguments.get("months", 6)
            part_name = arguments.get("part_name")
            spare_part_id = arguments.get("spare_part_id")
            center_id = arguments.get("center_id")
            
            # Filter out None values
            if part_name in [None, "None", ""]:
                part_name = None
            if spare_part_id in [None, "None", ""]:
                spare_part_id = None
            if center_id in [None, "None", ""]:
                center_id = None
            
            months = max(1, min(24, months))
            
            sql = """
            SELECT h.usageid, h.sparepartid, h.centerid, h.quantityused, h.useddate, h.status,
                   s.name as spare_part_name, s.unitprice, c.name as center_name
            FROM sparepartusagehistory_tuht h
            LEFT JOIN sparepart_tuht s ON h.sparepartid = s.sparepartid
            LEFT JOIN centertuantm c ON h.centerid = c.centerid
            WHERE h.useddate >= (now() - (%s::int * interval '1 month')) AND h.isactive = true
            """
            params = [months]
            
            if part_name:
                sql += " AND s.name ILIKE %s"
                params.append(f"%{part_name}%")
            if spare_part_id:
                sql += " AND h.sparepartid = %s"
                params.append(spare_part_id)
            if center_id:
                sql += " AND h.centerid = %s"
                params.append(center_id)
                
            sql += " ORDER BY h.useddate DESC LIMIT 15"
            rows = await fetch(sql, *params)
            # Real usage history from Supabase
            usage_data = []
            for row in rows[:10]:
                usage_data.append({
                    "usage_id": row.get("usageid", ""),
                    "date": str(row.get("useddate", "")),
                    "spare_part_id": row.get("sparepartid", ""),
                    "part_name": row.get("spare_part_name", ""),
                    "center_id": row.get("centerid", ""),
                    "center_name": row.get("center_name", ""),
                    "quantity_used": row.get("quantityused", 0),
                    "price": row.get("unitprice", 0),
                    "status": row.get("status", "")
                })
            return {"data": usage_data, "count": len(usage_data), "months": months, "source": "supabase_real_data"}
        
        elif function_name == "forecast_demand":
            months = arguments.get("months", 6)
            part_name = arguments.get("part_name")
            spare_part_id = arguments.get("spare_part_id")
            center_id = arguments.get("center_id")
            
            print(f"üîç Forecast arguments: months={months}, part_name='{part_name}', spare_part_id='{spare_part_id}', center_id='{center_id}'")
            
            # Filter out None values
            if part_name in [None, "None", ""]:
                part_name = None
            if spare_part_id in [None, "None", ""]:
                spare_part_id = None
            if center_id in [None, "None", ""]:
                center_id = None
            
            print(f"üîç After filtering: part_name='{part_name}', spare_part_id='{spare_part_id}'")
            
            try:
                # Use integrated forecast engine
                from ai_chatbot.forecast_engine import run_forecast_async
                
                # If part_name is provided, find spare_part_id first and ONLY forecast that part
                part_info = None
                if part_name and not spare_part_id:
                    find_sql = "SELECT sparepartid, name, unitprice, manufacture FROM sparepart_tuht WHERE name ILIKE %s AND isactive = true LIMIT 1"
                    find_rows = await fetch(find_sql, f"%{part_name}%")
                    if find_rows:
                        spare_part_id = find_rows[0].get("sparepartid")
                        part_info = find_rows[0]
                        print(f"üîç Found part: {part_info['name']} with ID: {spare_part_id}")
                    else:
                        return {
                            "forecast_months": months,
                            "forecast_result": [],
                            "part_info": None,
                            "searched_part_name": part_name,
                            "message": f"Kh√¥ng t√¨m th·∫•y ph·ª• t√πng '{part_name}' trong h·ªá th·ªëng"
                        }
                
                # IMPORTANT: When part_name is provided, MUST pass spare_part_id to forecast only that specific part
                forecast_result = await run_forecast_async(
                    spare_part_id=spare_part_id if part_name else None,  # Force specific part when name provided
                    center_id=center_id, 
                    forecast_months=max(1, min(12, months))
                )
                
                print(f"üîç Forecast called with spare_part_id: {spare_part_id if part_name else 'None (all parts)'}")
                
                # Get spare part info if not already retrieved
                if not part_info and spare_part_id:
                    part_sql = "SELECT name, unitprice, manufacture FROM sparepart_tuht WHERE sparepartid = %s"
                    part_rows = await fetch(part_sql, spare_part_id)
                    if part_rows:
                        part_info = part_rows[0]
                
                return {
                    "forecast_months": months,
                    "forecast_result": forecast_result,
                    "part_info": part_info,
                    "searched_part_name": part_name,
                    "data_source": "integrated_supabase_engine",
                    "message": f"D·ª± b√°o cho {months} th√°ng t·ªõi ƒë√£ ho√†n th√†nh"
                }
            except Exception as e:
                return {
                    "forecast_months": months,
                    "forecast_result": [],
                    "error": str(e),
                    "message": f"D·ª± b√°o cho {months} th√°ng t·ªõi - L·ªói h·ªá th·ªëng t√≠ch h·ª£p"
                }
        
        elif function_name == "create_sparepart":
            # Ph√¢n t√≠ch th√¥ng tin ƒë√£ c√≥ v√† c√≤n thi·∫øu
            provided_fields = {}
            missing_fields = []
            
            # C√°c tr∆∞·ªùng b·∫Øt bu·ªôc theo request body m·∫´u
            required_fields = {
                "name": "T√™n ph·ª• t√πng",
                "unitPrice": "Gi√° ƒë∆°n v·ªã", 
                "manufacturer": "Nh√† s·∫£n xu·∫•t",
                "typeName": "Lo·∫°i ph·ª• t√πng",
                "vehicleModelId": "ID m·∫´u xe",
                "centerName": "T√™n trung t√¢m",
                "description": "M√¥ t·∫£",
                "partNumber": "M√£ ph·ª• t√πng"
            }
            
            # Ki·ªÉm tra th√¥ng tin ƒë√£ cung c·∫•p
            for field, label in required_fields.items():
                value = arguments.get(field)
                if value and value not in [None, "None", ""]:
                    provided_fields[field] = {"label": label, "value": value}
                else:
                    missing_fields.append({"field": field, "label": label})
            
            return {
                "action": "create_sparepart_form",
                "provided_fields": provided_fields,
                "missing_fields": missing_fields,
                "message": "Th√¥ng tin t·∫°o ph·ª• t√πng m·ªõi",
                "note": "AI s·∫Ω kh√¥ng t·ª± ƒë·ªông t·∫°o v√†o database. Frontend c·∫ßn hi·ªÉn th·ªã form ƒë·ªÉ ng∆∞·ªùi d√πng nh·∫≠p ƒë·∫ßy ƒë·ªß th√¥ng tin."
            }
        
        else:
            return {"error": f"Unknown function: {function_name}"}
    
    def format_function_response(self, function_result: dict) -> str:
        """Format function result into readable response."""
        function_name = function_result.get("function", "")
        result = function_result.get("result", {})
        
        if function_name == "get_spare_parts":
            data = result.get("data", [])
            if not data:
                return "Kh√¥ng t√¨m th·∫•y ph·ª• t√πng n√†o."
            
            response = f"T√¨m th·∫•y {len(data)} ph·ª• t√πng:\n\n"
            for i, item in enumerate(data[:5], 1):
                response += f"{i}. {item.get('name', 'N/A')}\n"
                response += f"   - Gi√°: {item.get('price', 0):,.0f} VNƒê\n"
                response += f"   - H√£ng: {item.get('manufacture', 'N/A')}\n"
                response += f"   - S·ªë l∆∞·ª£ng: {item.get('qty', 0)}\n\n"
            return response
            
        elif function_name == "get_inventory":
            data = result.get("data", [])
            if not data:
                return "Kh√¥ng c√≥ d·ªØ li·ªáu t·ªìn kho."
            
            response = f"T√¨nh tr·∫°ng t·ªìn kho ({len(data)} m·∫∑t h√†ng):\n\n"
            for i, item in enumerate(data[:5], 1):
                response += f"{i}. {item.get('part_name', 'N/A')}\n"
                response += f"   - S·ªë l∆∞·ª£ng: {item.get('quantity', 0)}\n"
                response += f"   - M·ª©c t·ªëi thi·ªÉu: {item.get('min_stock', 0)}\n\n"
            return response
            
        else:
            return "ƒê√£ x·ª≠ l√Ω y√™u c·∫ßu th√†nh c√¥ng."
    
    async def process_chat_message(self, message: str, conversation_id: str, user_id: str = "anonymous", context: dict = None) -> dict:
        """Process a chat message with MCP function calling."""
        try:
            # Start MCP server if needed
            if self.mcp_process is None:
                self.start_mcp_server()
            
            # Get conversation context
            context_summary = self.conversation_manager.get_context_summary(conversation_id)
            
            # Minimal context to reduce tokens
            contextualized_message = f"{context_summary} {message}" if context_summary else message
            
            # Start chat session
            chat = self.model.start_chat()
            response = chat.send_message(contextualized_message)
            
            print(f"üîç Response candidates: {len(response.candidates)}")
            print(f"üîç Response finish_reason: {response.candidates[0].finish_reason if response.candidates else 'No candidates'}")
            
            # Check for blocked response
            if not response.candidates or response.candidates[0].finish_reason != 1:
                print(f"‚ö†Ô∏è Response blocked or invalid. Finish reason: {response.candidates[0].finish_reason if response.candidates else 'None'}")
                # Use fallback model immediately
                try:
                    fallback_response = self.fallback_model.generate_content(message)
                    return {
                        "success": True,
                        "response": fallback_response.text,
                        "mode": "fallback_due_to_blocked_response",
                        "conversation_id": conversation_id,
                        "timestamp": datetime.now().isoformat(),
                        "function_calls": [],
                        "function_results": [],
                        "function_call_count": 0
                    }
                except Exception as fallback_error:
                    return {
                        "success": False,
                        "error": f"Response blocked v√† fallback failed: {str(fallback_error)}",
                        "conversation_id": conversation_id,
                        "timestamp": datetime.now().isoformat()
                    }
            
            if response.candidates:
                print(f"üîç Content parts: {len(response.candidates[0].content.parts)}")
                for i, part in enumerate(response.candidates[0].content.parts):
                    print(f"üîç Part {i}: has_function_call={hasattr(part, 'function_call')}")
                    if hasattr(part, 'function_call'):
                        print(f"üîç Function call: {part.function_call}")
            
            # Handle function calls - LIMIT TO 1 CALL ONLY
            function_results = []
            max_calls = 1  # Only allow 1 function call
            call_count = 0
            
            while call_count < max_calls:
                # Check if response has function calls
                has_function_call = False
                
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'function_call') and part.function_call:
                        function_call = part.function_call
                        function_name = function_call.name
                        function_args = dict(function_call.args) if function_call.args else {}
                        call_count += 1
                        has_function_call = True
                        
                        print(f"üîß Function call: {function_name}({function_args})")
                        
                        # Call MCP function with error handling
                        try:
                            function_result = await self.call_mcp_function(function_name, function_args)
                            # Check if function returned error
                            if "error" in function_result:
                                print(f"‚ö†Ô∏è Function error: {function_result['error']}")
                                return {
                                    "success": False,
                                    "error": f"L·ªói function {function_name}: {function_result['error']}",
                                    "conversation_id": conversation_id,
                                    "timestamp": datetime.now().isoformat()
                                }
                            
                            function_results.append({
                                "function": function_name,
                                "args": function_args,
                                "result": function_result
                            })
                            
                            # Don't send back to Gemini, format response directly
                            print(f"‚úÖ Function executed successfully: {function_name}")
                        except Exception as func_error:
                            print(f"‚ö†Ô∏è Function call failed: {func_error}")
                            return {
                                "success": False,
                                "error": f"L·ªói g·ªçi function {function_name}: {str(func_error)}",
                                "conversation_id": conversation_id,
                                "timestamp": datetime.now().isoformat()
                            }
                        break
                
                if not has_function_call:
                    break
            
            # Get final response or fallback to generative chat
            try:
                # Check if response has valid text
                ai_response = ""
                if hasattr(response, 'text') and response.text:
                    ai_response = response.text
                elif response.candidates and len(response.candidates) > 0:
                    # Try to get text from candidates
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'content') and candidate.content and candidate.content.parts:
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                ai_response += part.text
                
                # If function was called, format response from function result
                if function_results and (not ai_response or len(ai_response.strip()) < 5):
                    ai_response = self.format_function_response(function_results[0])
                elif not ai_response or len(ai_response.strip()) < 5:
                    print("‚ö†Ô∏è No valid AI response, using fallback model")
                    try:
                        fallback_response = self.fallback_model.generate_content(message)
                        ai_response = fallback_response.text
                    except Exception as fallback_error:
                        print(f"‚ö†Ô∏è Fallback model also failed: {fallback_error}")
                        ai_response = "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau."
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Error getting response text: {e}")
                # Return error response instead of fallback
                return {
                    "success": False,
                    "error": f"L·ªói AI response: {str(e)}",
                    "conversation_id": conversation_id,
                    "timestamp": datetime.now().isoformat()
                }
            
            # Save to conversation history
            self.conversation_manager.add_message(
                conversation_id, "user", message
            )
            self.conversation_manager.add_message(
                conversation_id, "assistant", ai_response, 
                [f["function"] for f in function_results]
            )
            
            return {
                "success": True,
                "response": ai_response,
                "function_calls": [f["function"] for f in function_results],
                "function_results": function_results,
                "conversation_id": conversation_id,
                "user_id": user_id,
                "timestamp": datetime.now().isoformat(),
                "function_call_count": len(function_results)
            }
            
        except Exception as e:
            error_msg = f"L·ªói x·ª≠ l√Ω tin nh·∫Øn: {str(e)}"
            
            # Save error to conversation
            self.conversation_manager.add_message(
                conversation_id, "user", message
            )
            self.conversation_manager.add_message(
                conversation_id, "error", error_msg
            )
            
            return {
                "success": False,
                "error": error_msg,
                "conversation_id": conversation_id,
                "timestamp": datetime.now().isoformat()
            }
    
    def get_conversation_history(self, conversation_id: str) -> List[Dict]:
        """Get conversation history."""
        return self.conversation_manager.get_conversation(conversation_id)
    
    def close(self):
        """Clean up resources."""
        self.stop_mcp_server()